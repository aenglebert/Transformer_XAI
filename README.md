# XAI for transformers

- [Introduction](#Introduction)
- [Papers](#papers)
- [Repositories](#repositories)
- [Others](#others)

## Introduction

This repository aim to centralize references to ressources related to explainability applied to transformers models.

## Papers

- [Generic Attention-model Explainability for Interpreting Bi-Modal and Encoder-Decoder Transformers](https://arxiv.org/abs/2103.15679) Generic xai method applicable to a wide range of transformers, including multi-modal and encoder-decoder. (Video presentation: https://www.youtube.com/watch?v=bQTL34Dln-M)
- [Quantifying Attention Flow in Transformers](https://arxiv.org/abs/2005.00928)
- [Transformer Interpretability Beyond Attention Visualization](https://openaccess.thecvf.com/content/CVPR2021/html/Chefer_Transformer_Interpretability_Beyond_Attention_Visualization_CVPR_2021_paper.html)
- [XAI for Transformers: Better Explanations through Conservative Propagation](https://arxiv.org/abs/2202.07304)


## Repositories

- [XAI for Transformers: Better Explanations through Conservative Propagation - GitHub](https://github.com/ameenali/xai_transformers)


## Others
