# XAI for transformers

- [Introduction](#Introduction)
- [Papers](#papers)
- [Repositories](#repositories)
- [Others](#others)

## Introduction

This repository aim to centralize references to ressources related to explainability applied to transformers models.

## Papers

- [Generic Attention-model Explainability for Interpreting Bi-Modal and Encoder-Decoder Transformers](https://arxiv.org/abs/2103.15679) Generic xai method applicable to a wide range of transformers, including multi-modal and encoder-decoder. (Video presentation: https://www.youtube.com/watch?v=bQTL34Dln-M)
- [Quantifying Attention Flow in Transformers](https://arxiv.org/abs/2005.00928)
- [Transformer Interpretability Beyond Attention Visualization](https://openaccess.thecvf.com/content/CVPR2021/html/Chefer_Transformer_Interpretability_Beyond_Attention_Visualization_CVPR_2021_paper.html)
- [XAI for Transformers: Better Explanations through Conservative Propagation](https://arxiv.org/abs/2202.07304)
- [Explainable Sentiment Analysis: A Hierarchical Transformer-Based Extractive Summarization Approach ](https://www.semanticscholar.org/paper/Explainable-Sentiment-Analysis%3A-A-Hierarchical-Bacco-Cimino/afcf2e7ab4da89ee9dc8b8cda41412865ebc1bc5)
- [Do Vision Transformers See Like Convolutional Neural Networks?](https://arxiv.org/abs/2108.08810) even if it does not refer to XAI directly, the comparison modality between ViT and CNN can be interesting
-[What Does a Language-And-Vision Transformer See: The Impact of Semantic Information on Visual Representations](https://www.frontiersin.org/articles/10.3389/frai.2021.767971/full)
-[Attention is not an explanation](https://aclanthology.org/N19-1357.pdf)

## Tools
- [The Language Interpretability Tool (LIT): Interactive Exploration and Analysis of NLP Models](https://ai.googleblog.com/2020/11/the-language-interpretability-tool-lit.html)
- [InterpreT - GitHub](https://github.com/IntelLabs/nlp-architect/tree/master/solutions/InterpreT)
- [Transformers Interpret - GitHub](https://github.com/cdpierse/transformers-interpret) Transformers Interpret is a model explainability tool designed to work exclusively with the ðŸ¤— transformers package.
- [Ferret - GitHub](https://github.com/g8a9/ferret)
- [T3-Vis - GitHub](https://github.com/raymondzmc/T3-Vis) T3-Vis is a visual analytic framework designed to assist in the training and fine-tuning of Transformer-based
models.
- [VL-Interpret - GitHub](https://github.com/IntelLabs/VL-InterpreT)


## Demo
-[VL-InterpreT](http://vlinterpret38-env-2.eba-bgxp4fxk.us-east-2.elasticbeanstalk.com/) An Int
eractive Visualization Tool for Interpreting Vision-Language Transformers

## Repositories

- [XAI for Transformers: Better Explanations through Conservative Propagation - GitHub](https://github.com/ameenali/xai_transformers)
- [Explainability for Vision Transformers (in PyTorch)](https://github.com/jacobgil/vit-explain)
- [Awesome Visual-Transformer](https://github.com/dk-liang/Awesome-Visual-Transformer): Collect some Transformer with Computer-Vision (CV) papers.


## XAI with HCI

- [Principles of Explanatory Debugging to Personalize Interactive Machine Learning](https://dl.acm.org/doi/10.1145/2678025.2701399)
- [Using Explainability to Help Children UnderstandGender Bias in AI](https://dl.acm.org/doi/fullHtml/10.1145/3459990.3460719)
## Others
