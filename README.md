# XAI applied to transformers

- [Introduction](#Introduction)
- [Papers](#papers)
- [Repositories](#repositories)
- [Others](#others)

## Introduction

This repository aim to centralize references to ressources related to explainability applied to transformers models.

## Papers

- [Generic Attention-model Explainability for Interpreting Bi-Modal and Encoder-Decoder Transformers](https://arxiv.org/abs/2103.15679)
- [Quantifying Attention Flow in Transformers](https://arxiv.org/abs/2005.00928)
- [Transformer Interpretability Beyond Attention Visualization](https://openaccess.thecvf.com/content/CVPR2021/html/Chefer_Transformer_Interpretability_Beyond_Attention_Visualization_CVPR_2021_paper.html)
- [XAI for Transformers: Better Explanations through Conservative Propagation](https://arxiv.org/abs/2202.07304)


## Repositories


## Others
